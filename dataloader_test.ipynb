{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# opt.dir 分别改成train和test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "f_img   = '/disk1/home/yupeng/Data/K3OD/training/image_2/000111.png'\n",
    "f_label   = '/disk1/home/yupeng/Data/K3OD/training/label_2/000111.txt'\n",
    "# imgooo = mpimg.imread(f_img) \n",
    "# plt.imshow(imgooo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(f_label).readlines()\n",
    "lines\n",
    "# lines = [x.strip('\\n') for x in lines]\n",
    "items = [x.strip(' ').split(' ') for x in lines]\n",
    "items[1][]\n",
    "# items[0][4:8]\n",
    "# lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dir = '/disk1/home/yupeng/Data/K3OD/training/label_2/'\n",
    "labels = []\n",
    "for frame in range(0, 7481):\n",
    "\n",
    "    label_f = label_dir + '%06d.txt' % frame\n",
    "#     print(label_f)\n",
    "    lines = open(label_f).readlines()\n",
    "    lines = [x.strip('\\n') for x in lines]\n",
    "    items = [x.strip(' ').split(' ') for x in lines]\n",
    "#     print(len(lines))\n",
    "    for j in range(len(lines)):\n",
    "        new_label = items[j][0]\n",
    "        if new_label not in labels:\n",
    "            labels.append(new_label)\n",
    "            print(new_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 需要 BBox ('ymin', 'xmin', 'ymax', 'xmax')\n",
    "\n",
    "\n",
    "### 目前：第5-8列的599.41 156.40 629.75 189.25是目标的2D bounding box 像素位置，形式为xyxy，前两个值为bounding box左上点的x，y位置，后两个点为右下角的x,y位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bbox = list()\n",
    "label = list()\n",
    "difficult = list()\n",
    "depth = list()\n",
    "y_rot = list()\n",
    "\n",
    "xmin, ymin, xmax, ymax = items[0][4:8]\n",
    "bbox.append([int(float(ymin)), int(float(xmin)), int(float(ymax)), int(float(xmax))])\n",
    "label.append(items[0][0])\n",
    "difficult.append(False)\n",
    "depth.append(float(items[0][13]))\n",
    "y_rot.append(float(items[0][3]))\n",
    "\n",
    "bbox = np.stack(bbox).astype(np.float32)\n",
    "label = np.stack(label).astype(np.int32)\n",
    "print(type(difficult))\n",
    "difficult = np.array(difficult, dtype=np.bool).astype(np.uint8)\n",
    "print(type(difficult))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "def pytorch_normalze(img):\n",
    "    \"\"\"\n",
    "    https://github.com/pytorch/vision/issues/223\n",
    "    return appr -1~1 RGB\n",
    "    \"\"\"\n",
    "    normalize = tvtsf.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "    img = normalize(t.from_numpy(img))\n",
    "    return img.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test inverse_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = Image.open(f_img)\n",
    "img = f.convert('RGB')\n",
    "img = np.asarray(img, dtype=np.float32)\n",
    "img = img.transpose((2, 0, 1))\n",
    "img.shape\n",
    "img = img /255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(img.min())\n",
    "print(img.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn_img = pytorch_normalze(img)\n",
    "print(pn_img.max())\n",
    "print(pn_img.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, H, W = img.shape\n",
    "img, params = util.random_flip(img, x_random=True, return_param=True)\n",
    "_, o_H, o_W = img.shape\n",
    "scale = o_H / H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = util.flip_bbox(bbox, (o_H, o_W), x_flip=params['x_flip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bbox (~numpy.ndarray): An array whose shape is :math:`(R, 4)`.\n",
    "#             :math:`R` is the number of bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from data.util import read_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ImageKITTIDataset('/disk1/home/yupeng/Data/temp2D/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_img, bbox, label, difficult, depth, y_rot = a.get_example(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import  absolute_import\n",
    "from __future__ import  division\n",
    "import torch as t\n",
    "from skimage import transform as sktsf\n",
    "from torchvision import transforms as tvtsf\n",
    "from data import util\n",
    "import numpy as np\n",
    "from utils.config import opt\n",
    "from math import pi\n",
    "from data.kitti_img_dataset import ImageKITTIDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_normalize(img):\n",
    "    # approximate un-normalize for visualize\n",
    "    return (img * 0.225 + 0.45).clip(min=0, max=1) * 255\n",
    "\n",
    "\n",
    "def pytorch_normalze(img):\n",
    "    \"\"\"\n",
    "    https://github.com/pytorch/vision/issues/223\n",
    "    return appr -1~1 RGB\n",
    "    \"\"\"\n",
    "    normalize = tvtsf.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "    img = normalize(t.from_numpy(img))\n",
    "    return img.numpy()\n",
    "\n",
    "def preprocess(img, min_size=375, max_size=1242):\n",
    "    \"\"\"Preprocess an image for feature extraction.\n",
    "    The length of the shorter edge is scaled to :obj:`self.min_size`.\n",
    "    After the scaling, if the length of the longer edge is longer than\n",
    "    :param min_size:\n",
    "    :obj:`self.max_size`, the image is scaled to fit the longer edge\n",
    "    to :obj:`self.max_size`.\n",
    "    After resizing the image, the image is subtracted by a mean image value\n",
    "    :obj:`self.mean`.\n",
    "    Args:\n",
    "        img (~numpy.ndarray): An image. This is in CHW and RGB format.\n",
    "            The range of its value is :math:`[0, 255]`.\n",
    "    Returns:\n",
    "        ~numpy.ndarray: A preprocessed image.\n",
    "    \"\"\"\n",
    "    C, H, W = img.shape\n",
    "    scale1 = min_size / min(H, W)\n",
    "    scale2 = max_size / max(H, W)\n",
    "    scale = min(scale1, scale2)\n",
    "    img = img / 255.\n",
    "    img = sktsf.resize(img, (C, H * scale, W * scale), mode='reflect',anti_aliasing=False)\n",
    "    # both the longer and shorter should be less than\n",
    "    # max_size and min_size\n",
    "    normalize = pytorch_normalze\n",
    "    return normalize(img)\n",
    "\n",
    "class Transform(object):\n",
    "\n",
    "    def __init__(self, min_size=375, max_size=1242):\n",
    "        self.min_size = min_size\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, in_data):\n",
    "        img, bbox, label, depth, y_rot = in_data\n",
    "        _, H, W = img.shape\n",
    "        img = preprocess(img, self.min_size, self.max_size)\n",
    "        _, o_H, o_W = img.shape\n",
    "        scale = o_H / H\n",
    "        bbox = util.resize_bbox(bbox, (H, W), (o_H, o_W))\n",
    "\n",
    "        # horizontally flip\n",
    "        img, params = util.random_flip(\n",
    "            img, x_random=True, return_param=True)\n",
    "        bbox = util.flip_bbox(\n",
    "            bbox, (o_H, o_W), x_flip=params['x_flip'])\n",
    "        print(params['x_flip'])\n",
    "        if params['x_flip']:\n",
    "            print(y_rot)\n",
    "            for i in range(len(y_rot)):\n",
    "                theta = float(y_rot[i])\n",
    "                if theta > 0:\n",
    "                    y_rot[i] = pi - theta\n",
    "                if theta < 0:\n",
    "                    y_rot[i] = -pi - theta\n",
    "                    y_rot[i]\n",
    "                if theta == 0:\n",
    "                    y_rot[i] = 3.14\n",
    "\n",
    "        return img, bbox, label, depth, y_rot, scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = ImageKITTIDataset('/disk1/home/yupeng/Data/temp2D/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000066\n"
     ]
    }
   ],
   "source": [
    "ori_img, bbox, label, difficult, depth, y_rot = db.get_example(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[109., 110., 123., ...,   8.,   8.,   6.],\n",
       "         [110., 108., 118., ...,   4.,   5.,   6.],\n",
       "         [118., 107., 164., ...,   6.,   7.,   7.],\n",
       "         ...,\n",
       "         [ 15.,  15.,  16., ...,  12.,  13.,  13.],\n",
       "         [ 16.,  16.,  16., ...,  12.,  12.,  13.],\n",
       "         [ 14.,  14.,  15., ...,  13.,  13.,  13.]],\n",
       " \n",
       "        [[177., 178., 178., ...,   8.,   6.,   6.],\n",
       "         [175., 176., 180., ...,   6.,   7.,  10.],\n",
       "         [172., 175., 177., ...,   8.,  13.,  21.],\n",
       "         ...,\n",
       "         [ 17.,  17.,  18., ...,  13.,  13.,  11.],\n",
       "         [ 20.,  20.,  20., ...,  14.,  13.,  12.],\n",
       "         [ 18.,  18.,  18., ...,  15.,  14.,  13.]],\n",
       " \n",
       "        [[255., 255., 255., ...,   5.,   6.,   6.],\n",
       "         [255., 255., 255., ...,   6.,   6.,   6.],\n",
       "         [255., 255., 255., ...,   7.,   6.,   6.],\n",
       "         ...,\n",
       "         [ 15.,  17.,  20., ...,  13.,  13.,  13.],\n",
       "         [ 24.,  26.,  27., ...,  13.,  13.,  13.],\n",
       "         [ 27.,  27.,  27., ...,  13.,  13.,  13.]]], dtype=float32),\n",
       " array([[168., 671., 206., 710.],\n",
       "        [172., 591., 199., 633.]], dtype=float32),\n",
       " array([0, 0], dtype=int32),\n",
       " array([0, 0], dtype=uint8),\n",
       " array([0.45114285, 0.5855714 ], dtype=float32),\n",
       " array([-1.53,  1.75], dtype=float32))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_img, bbox, label, difficult, depth, y_rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = preprocess(ori_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, min_size=375, max_size=1242):\n",
    "    \"\"\"Preprocess an image for feature extraction.\n",
    "    The length of the shorter edge is scaled to :obj:`self.min_size`.\n",
    "    After the scaling, if the length of the longer edge is longer than\n",
    "    :param min_size:\n",
    "    :obj:`self.max_size`, the image is scaled to fit the longer edge\n",
    "    to :obj:`self.max_size`.\n",
    "    After resizing the image, the image is subtracted by a mean image value\n",
    "    :obj:`self.mean`.\n",
    "    Args:\n",
    "        img (~numpy.ndarray): An image. This is in CHW and RGB format.\n",
    "            The range of its value is :math:`[0, 255]`.\n",
    "    Returns:\n",
    "        ~numpy.ndarray: A preprocessed image.\n",
    "    \"\"\"\n",
    "    C, H, W = img.shape\n",
    "    scale1 = min_size / min(H, W)\n",
    "    scale2 = max_size / max(H, W)\n",
    "    scale = min(scale1, scale2)\n",
    "    img = img / 255.\n",
    "    img = sktsf.resize(img, (C, H * scale, W * scale), mode='reflect',anti_aliasing=False)\n",
    "    # both the longer and shorter should be less than\n",
    "    # max_size and min_size\n",
    "    normalize = pytorch_normalze\n",
    "    return normalize(img)\n",
    "def pytorch_normalze(img):\n",
    "    \"\"\"\n",
    "    https://github.com/pytorch/vision/issues/223\n",
    "    return appr -1~1 RGB\n",
    "    \"\"\"\n",
    "    normalize = tvtsf.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "    img = normalize(t.from_numpy(img))\n",
    "    return img.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(375, 1242)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_img_t = ori_img/255\n",
    "plt.imshow(ori_img_t.transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nimg_t = inverse_normalize(nimg)/255\n",
    "plt.imshow(nimg_t.transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bbox)\n",
    "test_img = ori_img[:, 168:206,  671:710]\n",
    "test_img_t = test_img/255\n",
    "plt.imshow(test_img_t.transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nbbox)\n",
    "test_img = nimg[:, 168:206,  532:571]\n",
    "test_img_t = inverse_normalize(test_img)/255\n",
    "plt.imshow(test_img_t.transpose((1, 2, 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label)\n",
    "print(nlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_rot)\n",
    "print(ny_rot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scn",
   "language": "python",
   "name": "scn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
